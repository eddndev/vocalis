# Informe de Sesión: Corrección del Pipeline DSP y Estandarización Global
**Fecha:** 6 de Diciembre, 2025
**Proyecto:** Vocalis (Reconocimiento de Vocales mediante MFCC + SVM en Rust/WASM)
**Autor:** (Assistant) & Eduardo (User)

---

## 1. Resumen Ejecutivo

En esta sesión, hemos diagnosticado y resuelto una falla crítica en la arquitectura de procesamiento de señales (DSP) del proyecto Vocalis. El sistema, diseñado para clasificar vocales sostenidas utilizando Machine Learning (SVM), presentaba una discrepancia fundamental entre el entorno de entrenamiento (Python) y el entorno de inferencia (Rust/WASM).

El síntoma observado eran vectores de características (MFCCs) con valores cercanos a cero o inconsistentes, lo que impedía una validación cruzada efectiva. Tras un análisis profundo de la metodología y el código, identificamos la causa raíz: un error conceptual en la aplicación de la **Normalización de Media Cepstral (CMN)** sobre señales estacionarias (vocales sostenidas). Esto causaba que la información fonética fuera eliminada matemáticamente durante la extracción de características.

Se ha implementado una nueva estrategia de **Estandarización Global**, modificando tanto el pipeline de entrenamiento en Python como el motor de inferencia en Rust. Se ha verificado mediante pruebas unitarias que la nueva extracción de características preserva la información de la señal. Actualmente, el sistema se encuentra reconstruyendo el dataset completo con esta nueva lógica.

---

## 2. Diagnóstico del Problema: La "Paradoja de la Vocal Estacionaria"

### 2.1. Síntomas Iniciales
Al intentar validar los vectores generados por el motor de Rust (`vocalis_core`) comparándolos con los scripts de referencia en Python (`debug_mfcc_values.py`), observamos dos fenómenos contradictorios:
1.  Al usar una señal de prueba constante (seno de 150Hz), los resultados tendían a cero.
2.  Al inspeccionar el dataset de entrenamiento (`dsp_features.csv`), los valores almacenados para los coeficientes MFCC eran del orden de `1e-7` (esencialmente cero).

### 2.2. Análisis Matemático
La metodología original seguía el siguiente flujo para cada archivo de audio:
1.  Extracción de MFCCs (Matriz `[13, T]`).
2.  **CMN Local**: Restar la media temporal a cada coeficiente ($x_t' = x_t - \mu_{file}$).
3.  **Bag-of-Frames**: Calcular el promedio temporal del resultado ($\text{Feature} = \text{Mean}(x_t')$).

**El Error Conceptual:**
La CMN está diseñada para eliminar el sesgo estacionario del canal (micrófono) en señales que *varían* fonéticamente (frases completas). Sin embargo, una vocal sostenida (ej. "aaaaa") es, por definición, una señal **estacionaria**.
*   En una señal estacionaria, el valor instantáneo es aproximadamente igual a su media temporal ($x_t \approx \mu_{file}$).
*   Por lo tanto, la operación $x_t - \mu_{file}$ resulta en valores cercanos a cero.
*   Al promediar estos ceros ("Bag-of-Frames"), el vector de características resultante es nulo.

Matemáticamente:
$$ \text{Feature} = \frac{1}{T} \sum_{t=1}^{T} (x_t - \frac{1}{T}\sum_{k=1}^{T} x_k) = 0 $$

Esto explica por qué el dataset estaba "vacío" de información a pesar de tener miles de archivos, y por qué el modelo entrenado era incapaz de generalizar correctamente o dependía de artefactos numéricos marginales.

---

## 3. Solución Técnica: Arquitectura "Opción A"

Para resolver esto sin renunciar a la normalización (necesaria para el SVM), hemos migrado a una estrategia de **Estandarización Global**.

### 3.1. Nueva Estrategia de Normalización
En lugar de normalizar cada archivo contra su propia media (lo cual borra la identidad de la vocal), normalizamos cada archivo contra las estadísticas globales de **todo el corpus de entrenamiento**.

1.  **Extracción (Python/Rust):** Se calculan los MFCCs crudos ("Raw MFCCs"). Estos valores dependen del volumen y del micrófono, pero preservan intacta la forma espectral de la vocal.
2.  **Entrenamiento (Python):**
    *   Se recolectan todos los vectores crudos del dataset.
    *   Se calcula la **Media Global** ($\mu_{global}$) y la **Desviación Estándar Global** ($\sigma_{global}$).
    *   Se entrena el SVM con los datos estandarizados: $z = \frac{x - \mu_{global}}{\sigma_{global}}$.
3.  **Inferencia (Rust):**
    *   El motor DSP extrae los MFCCs crudos del micrófono.
    *   Antes de pasar los datos al modelo, aplica la transformación lineal usando $\mu_{global}$ y $\sigma_{global}$ (que se cargan desde el archivo del modelo `vocalis_model.json`).

Esta estrategia es robusta porque la "referencia" ($\mu_{global}$) es fija y representa el "centro acústico" de todas las voces, permitiendo que la desviación de una vocal específica ("a", "i", "u") sea significativa y medible.

---

## 4. Cambios Implementados en el Código

Hemos realizado modificaciones quirúrgicas en ambos lados del stack tecnológico para garantizar la alineación.

### 4.1. Refactorización en Python (`research/dsp_lab`)

#### `feature_extractor.py`
Se eliminó la etapa de CMN local. Ahora la función `get_features` retorna el promedio de los coeficientes crudos.

```python
# ANTES (Incorrecto para vocales)
# mfccs_cmn = mfccs - np.mean(mfccs, axis=1, keepdims=True)
# mfccs_mean = np.mean(mfccs_cmn, axis=1)

# AHORA (Correcto)
mfccs_mean = np.mean(mfccs, axis=1)
```

#### `build_dsp_dataset.py`
Se corrigieron las rutas relativas para permitir su ejecución desde la raíz del proyecto y se preparó para regenerar el archivo `dsp_features.csv` con la nueva lógica de extracción.

#### `export_to_json.py`
Se verificó que este script ya exporta correctamente los parámetros `mean` y `scale` del objeto `StandardScaler` de Scikit-Learn. No fueron necesarios cambios adicionales aquí, ya que la infraestructura para la "Opción A" ya existía, solo necesitaba datos correctos.

### 4.2. Refactorización en Rust (`vocalis_core`)

#### `src/dsp.rs` (Motor de Señal)
Se eliminó la llamada a `cepstral_mean_normalization` dentro del método `extract_features`. Esto alinea el comportamiento con el nuevo `feature_extractor.py`, produciendo vectores crudos.

```rust
// src/dsp.rs
pub fn extract_features(...) -> Vec<f32> {
    // ... Pasos FFT, Mel, DCT ...
    let mfccs = self.dct(&log_mel_energies);
    
    // CORRECCIÓN: Se eliminó la normalización local
    // self.cepstral_mean_normalization(&mut mfccs); 
    
    mfccs
}
```

#### `src/lib.rs` (Lógica Principal)
1.  **Limpieza de Debug:** Se comentó el bloque de código que inyectaba una onda senoidal sintética de 150Hz. Esto restaura la funcionalidad del micrófono real para la aplicación final.
2.  **Validación de Flujo:** Se confirmó que el flujo `Audio -> DspProcessor (Raw) -> Predictor -> SVM` es correcto.

#### `src/inference.rs` (Predicción)
Se verificó que el método `predict` aplica la normalización global antes de computar los kernels del SVM:
```rust
pub fn normalize(features: &[f32], model: &GenderModel) -> Vec<f32> {
    features.iter()
        .zip(model.scaler.mean.iter())
        .zip(model.scaler.scale.iter())
        .map(|((&x, &m), &s)| (x - m) / s)
        .collect()
}
```
Esto confirma que la arquitectura en Rust está lista para recibir los parámetros globales.

---

## 5. Verificación de la Solución

Antes de proceder con el re-entrenamiento masivo, realizamos una **Prueba de Concepto (PoC)** crítica.

1.  **Estado Anterior:** `head dsp_features.csv` mostraba valores como `4.76e-07`, confirmando la corrupción de datos.
2.  **Prueba Unitaria:** Creamos un script temporal `test_extract_one.py` que aplicó el nuevo `feature_extractor.py` sobre un archivo real (`s001_M_a_0000.wav`).
3.  **Resultados Obtenidos:**
    ```
    [-523.05, 81.03, 62.79, ...]
    ```
    Estos son valores MFCC crudos, típicos y saludables. Contienen información real sobre la energía y la forma espectral.
4.  **Conclusión:** La corrección en el software es efectiva. El paso limitante ahora es computacional (procesar el dataset completo).

---

## 6. Estado Actual y Próximos Pasos

El sistema se encuentra en un estado de **espera de procesamiento**. El código está corregido, pero el cerebro del modelo (los pesos SVM y los escaladores) está desactualizado y basado en datos erróneos.

### Lista de Tareas para Retomar (Roadmap)

Cuando el script `build_dsp_dataset.py` finalice (aprox. 2 horas), debes ejecutar estrictamente la siguiente secuencia de comandos para materializar la solución:

#### Paso 1: Re-entrenamiento del Modelo
Entrenará los SVMs (Masculino/Femenino) usando los nuevos datos crudos. El script aplica automáticamente `StandardScaler`, calculando las nuevas medias globales necesarias para Rust.
```powershell
python research/dsp_lab/train_classifier.py
```
*Salida esperada:* Archivos `.pkl` actualizados en `research/dsp_lab/models/`.

#### Paso 2: Exportación de Pesos
Convertirá los modelos binarios de Python (PKL) a un formato portable (JSON) que Rust puede leer, incluyendo los vectores de soporte y los nuevos parámetros de normalización.
```powershell
python research/dsp_lab/export_to_json.py
```
*Salida esperada:* `research/dsp_lab/models/vocalis_model.json` actualizado.

#### Paso 3: Recompilación de Vocalis Core
Reconstruirá el binario WebAssembly, incrustando el nuevo archivo JSON.
```powershell
cd vocalis_core
wasm-pack build --target web
```
*Salida esperada:* Carpeta `pkg/` actualizada sin errores.

#### Paso 4: Pruebas de Integración (Frontend)
Una vez recompilado, el frontend web debería ser capaz de clasificar vocales reales desde el micrófono con una precisión similar a la reportada en Python (esperado >90%), ya que ahora ambos "hablan el mismo idioma" matemático.

---

## 7. Notas Adicionales y Futuro
*   **Gestión de Rutas:** Se corrigieron rutas relativas en los scripts de Python. De ahora en adelante, ejecutar siempre desde la raíz del proyecto (`c:\dev\vocalis`).
*   **Cost-Sensitive Learning:** Para futuras iteraciones, si se detecta un sesgo contra la vocal 'u', se recomienda activar `class_weight='balanced'` en `train_classifier.py`.
*   **Depuración:** Si se requiere volver a depurar, el script `debug_mfcc_values.py` ha sido ajustado para reflejar los valores target correctos (Raw MFCCs).

---

## 8. Actualización Crítica Post-Prueba: El Misterio de n_mels

Tras la primera prueba en navegador, a pesar de tener una precisión de entrenamiento del 95%, el modelo fallaba aleatoriamente en inferencia (predicciones cruzadas caóticas).

**Diagnóstico Final:**
Se descubrió una discrepancia fundamental en la configuración del banco de filtros:
*   **Python (Librosa Default):** Usaba `n_mels=128`.
*   **Rust (dsp.rs):** Usaba `n_mels=40`.

Esta diferencia hace que los coeficientes MFCC representen regiones espectrales totalmente distintas, haciendo que el modelo SVM sea "ciego" ante los datos de Rust.

**Solución Aplicada:**
1.  Se forzó `n_mels=40` en `feature_extractor.py`.
2.  **Acción Requerida:** Es obligatorio regenerar el dataset, reentrenar y reexportar una vez más. Esta vez, la alineación matemática será total.

**Comandos Definitivos para Recuperación:**
```powershell
# 1. Regenerar Dataset (Lento ~2h)
python research/dsp_lab/build_dsp_dataset.py

# 2. Entrenar (Rápido)
python research/dsp_lab/train_classifier.py

# 3. Exportar
python research/dsp_lab/export_to_json.py

# 4. Compilar
cd vocalis_core
wasm-pack build --target web
```
